test compile precise-output
target aarch64

function %f1() winch {
block0:
    return
}

; VCode:
; block0:
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ret

function %f2(i64, i64, i64, i64, i64, i64) -> i64 winch {
  sig0 = () winch
  fn0 = %g sig0

block0(v0:i64, v1:i64, v2:i64, v3:i64, v4:i64, v5:i64):
  call fn0()
  return v0
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   str x2, [sp]
;   load_ext_name x1, TestCase(%g)+0
;   blr x1
;   ldr x2, [sp]
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   stur x2, [sp]
;   ldr x1, #0x18
;   b #0x20
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %g 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   blr x1
;   ldur x2, [sp]
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %f3(i64, i64, i64, i64, i64, i64) -> i64 {
  sig0 = () winch
  fn0 = %g sig0

block0(v0:i64, v1:i64, v2:i64, v3:i64, v4:i64, v5:i64):
  call fn0()
  return v0
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   stp x27, x28, [sp, #-16]!
;   stp x25, x26, [sp, #-16]!
;   stp x23, x24, [sp, #-16]!
;   stp x21, x22, [sp, #-16]!
;   stp x19, x20, [sp, #-16]!
;   stp d14, d15, [sp, #-16]!
;   stp d12, d13, [sp, #-16]!
;   stp d10, d11, [sp, #-16]!
;   stp d8, d9, [sp, #-16]!
;   sub sp, sp, #16
; block0:
;   str x0, [sp]
;   load_ext_name x1, TestCase(%g)+0
;   blr x1
;   ldr x0, [sp]
;   add sp, sp, #16
;   ldp d8, d9, [sp], #16
;   ldp d10, d11, [sp], #16
;   ldp d12, d13, [sp], #16
;   ldp d14, d15, [sp], #16
;   ldp x19, x20, [sp], #16
;   ldp x21, x22, [sp], #16
;   ldp x23, x24, [sp], #16
;   ldp x25, x26, [sp], #16
;   ldp x27, x28, [sp], #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   stp x27, x28, [sp, #-0x10]!
;   stp x25, x26, [sp, #-0x10]!
;   stp x23, x24, [sp, #-0x10]!
;   stp x21, x22, [sp, #-0x10]!
;   stp x19, x20, [sp, #-0x10]!
;   stp d14, d15, [sp, #-0x10]!
;   stp d12, d13, [sp, #-0x10]!
;   stp d10, d11, [sp, #-0x10]!
;   stp d8, d9, [sp, #-0x10]!
;   sub sp, sp, #0x10
; block1: ; offset 0x30
;   stur x0, [sp]
;   ldr x1, #0x3c
;   b #0x44
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %g 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   blr x1
;   ldur x0, [sp]
;   add sp, sp, #0x10
;   ldp d8, d9, [sp], #0x10
;   ldp d10, d11, [sp], #0x10
;   ldp d12, d13, [sp], #0x10
;   ldp d14, d15, [sp], #0x10
;   ldp x19, x20, [sp], #0x10
;   ldp x21, x22, [sp], #0x10
;   ldp x23, x24, [sp], #0x10
;   ldp x25, x26, [sp], #0x10
;   ldp x27, x28, [sp], #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %f4(i64, i64, i64, i64, i64, i64) -> i64 winch {
  sig0 = (i64, i64, i64, i64, i64, i64) -> i64 winch
  fn0 = %g sig0

block0(v0:i64, v1:i64, v2:i64, v3:i64, v4:i64, v5:i64):
  v6 = call fn0(v5, v1, v2, v3, v4, v0)
  return v6
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
; block0:
;   mov x0, x2
;   mov x2, x7
;   mov x7, x0
;   load_ext_name x1, TestCase(%g)+0
;   blr x1
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
; block1: ; offset 0x8
;   mov x0, x2
;   mov x2, x7
;   mov x7, x0
;   ldr x1, #0x1c
;   b #0x24
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %g 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   blr x1
;   ldp x29, x30, [sp], #0x10
;   ret

function %f5(i64, i64, i64, i64, i64, i64) -> i64 {
  sig0 = (i64, i64, i64, i64, i64, i64) -> i64 winch
  fn0 = %g sig0

block0(v0:i64, v1:i64, v2:i64, v3:i64, v4:i64, v5:i64):
  v6 = call fn0(v5, v1, v2, v3, v4, v0)
  return v6
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   stp x27, x28, [sp, #-16]!
;   stp x25, x26, [sp, #-16]!
;   stp x23, x24, [sp, #-16]!
;   stp x21, x22, [sp, #-16]!
;   stp x19, x20, [sp, #-16]!
;   stp d14, d15, [sp, #-16]!
;   stp d12, d13, [sp, #-16]!
;   stp d10, d11, [sp, #-16]!
;   stp d8, d9, [sp, #-16]!
; block0:
;   mov x7, x0
;   mov x6, x4
;   mov x4, x2
;   mov x2, x5
;   mov x5, x3
;   mov x3, x1
;   load_ext_name x1, TestCase(%g)+0
;   blr x1
;   mov x0, x2
;   ldp d8, d9, [sp], #16
;   ldp d10, d11, [sp], #16
;   ldp d12, d13, [sp], #16
;   ldp d14, d15, [sp], #16
;   ldp x19, x20, [sp], #16
;   ldp x21, x22, [sp], #16
;   ldp x23, x24, [sp], #16
;   ldp x25, x26, [sp], #16
;   ldp x27, x28, [sp], #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   stp x27, x28, [sp, #-0x10]!
;   stp x25, x26, [sp, #-0x10]!
;   stp x23, x24, [sp, #-0x10]!
;   stp x21, x22, [sp, #-0x10]!
;   stp x19, x20, [sp, #-0x10]!
;   stp d14, d15, [sp, #-0x10]!
;   stp d12, d13, [sp, #-0x10]!
;   stp d10, d11, [sp, #-0x10]!
;   stp d8, d9, [sp, #-0x10]!
; block1: ; offset 0x2c
;   mov x7, x0
;   mov x6, x4
;   mov x4, x2
;   mov x2, x5
;   mov x5, x3
;   mov x3, x1
;   ldr x1, #0x4c
;   b #0x54
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %g 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   blr x1
;   mov x0, x2
;   ldp d8, d9, [sp], #0x10
;   ldp d10, d11, [sp], #0x10
;   ldp d12, d13, [sp], #0x10
;   ldp d14, d15, [sp], #0x10
;   ldp x19, x20, [sp], #0x10
;   ldp x21, x22, [sp], #0x10
;   ldp x23, x24, [sp], #0x10
;   ldp x25, x26, [sp], #0x10
;   ldp x27, x28, [sp], #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function u1:0() system_v {
    sig0 = () winch
    fn0 = u2:0 sig0

block0:
    v5 = func_addr.i64 fn0
    call_indirect sig0, v5()
    call_indirect sig0, v5()
    return
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   stp x27, x28, [sp, #-16]!
;   stp x25, x26, [sp, #-16]!
;   stp x23, x24, [sp, #-16]!
;   stp x21, x22, [sp, #-16]!
;   stp x19, x20, [sp, #-16]!
;   stp d14, d15, [sp, #-16]!
;   stp d12, d13, [sp, #-16]!
;   stp d10, d11, [sp, #-16]!
;   stp d8, d9, [sp, #-16]!
;   sub sp, sp, #16
; block0:
;   load_ext_name x1, User(userextname0)+0
;   str x1, [sp]
;   ldr x1, [sp]
;   blr x1
;   ldr x1, [sp]
;   blr x1
;   add sp, sp, #16
;   ldp d8, d9, [sp], #16
;   ldp d10, d11, [sp], #16
;   ldp d12, d13, [sp], #16
;   ldp d14, d15, [sp], #16
;   ldp x19, x20, [sp], #16
;   ldp x21, x22, [sp], #16
;   ldp x23, x24, [sp], #16
;   ldp x25, x26, [sp], #16
;   ldp x27, x28, [sp], #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   stp x27, x28, [sp, #-0x10]!
;   stp x25, x26, [sp, #-0x10]!
;   stp x23, x24, [sp, #-0x10]!
;   stp x21, x22, [sp, #-0x10]!
;   stp x19, x20, [sp, #-0x10]!
;   stp d14, d15, [sp, #-0x10]!
;   stp d12, d13, [sp, #-0x10]!
;   stp d10, d11, [sp, #-0x10]!
;   stp d8, d9, [sp, #-0x10]!
;   sub sp, sp, #0x10
; block1: ; offset 0x30
;   ldr x1, #0x38
;   b #0x40
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 u2:0 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   stur x1, [sp]
;   ldur x1, [sp]
;   blr x1
;   ldur x1, [sp]
;   blr x1
;   add sp, sp, #0x10
;   ldp d8, d9, [sp], #0x10
;   ldp d10, d11, [sp], #0x10
;   ldp d12, d13, [sp], #0x10
;   ldp d14, d15, [sp], #0x10
;   ldp x19, x20, [sp], #0x10
;   ldp x21, x22, [sp], #0x10
;   ldp x23, x24, [sp], #0x10
;   ldp x25, x26, [sp], #0x10
;   ldp x27, x28, [sp], #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

